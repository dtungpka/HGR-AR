{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these 2 cells first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split to images data and skeleton data\n",
    "#images > folder\n",
    "#skeleton > h5\n",
    "HEIGHT = 480\n",
    "WIDTH = 640\n",
    "Actions = ['Idle',\n",
    "           'Pickup_item',\n",
    "           'Use_item',\n",
    "           'Aim',\n",
    "           'Shoot'\n",
    "           ]\n",
    "def get_skeleton_json(json_file):\n",
    "    landmarks = []\n",
    "    handness = []\n",
    "    frame_ids = []\n",
    "    #labels \n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "        for frame in data:\n",
    "            for hand in data[frame]:\n",
    "                keypoint = data[frame][hand]['landmarks']\n",
    "                keypoint_arr = np.array([[kp[1],kp[2]] for kp in keypoint])\n",
    "                landmarks.append(keypoint_arr)\n",
    "                handness.append(int(hand))\n",
    "                frame_ids.append(int(frame))\n",
    "    labels = -1\n",
    "    for i in range(len(Actions)):\n",
    "        if Actions[i] in json_file:\n",
    "            labels = [i] * len(landmarks)\n",
    "            break\n",
    "    if labels == -1:\n",
    "        print(\"Error: No label found\")\n",
    "        return None, None, None\n",
    "    return landmarks, handness, labels, frame_ids\n",
    "\n",
    "def extract_numbers(s):\n",
    "    return re.findall(r'\\d+', s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and dataset creation (skeleton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:05<00:00,  7.71it/s]\n",
      "/tmp/ipykernel_292228/2578764674.py:37: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block1_values] [items->Index(['landmarks', 'fid'], dtype='object')]\n",
      "\n",
      "  pd_database.to_hdf(save_path, key='df', mode='w')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_folder(folder,pd_database):\n",
    "    if not len(os.listdir(folder)):\n",
    "        return None\n",
    "    elif len(os.listdir(folder)) == 1:\n",
    "        folder = os.path.join(folder, os.listdir(folder)[0])\n",
    "    #print(f'Processing folder {folder}')\n",
    "    base_name = os.path.basename(folder)\n",
    "    #process json\n",
    "    fid = str(extract_numbers(base_name)[0])[-4:]\n",
    "    #print(os.listdir(folder))\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith('.json') and 'detail' not in file:\n",
    "            landmarks, handness, labels, frame_ids = get_skeleton_json(os.path.join(folder, file))\n",
    "            if landmarks is None:\n",
    "                continue\n",
    "            #append to pd_database with columns: landmarks, handness, labels, fid\n",
    "            pd_database.append(pd.DataFrame({'landmarks':landmarks,\n",
    "                                             'handness':handness,\n",
    "                                             'labels':labels,\n",
    "                                             'fid':fid,\n",
    "                                             'frame_ids':frame_ids}))\n",
    "            #print(f'Processing {file}')\n",
    "    return pd_database\n",
    "\n",
    "def process_folders(data_folder):\n",
    "    pd_database = []\n",
    "    for folder in tqdm(os.listdir(data_folder)):\n",
    "        #if folder is a folder\n",
    "        if os.path.isdir(os.path.join(data_folder, folder)) and not '[discarded]' in folder:\n",
    "            _ = process_folder(os.path.join(data_folder, folder),pd_database)\n",
    "            if _ is not None:\n",
    "                pd_database = _\n",
    "    return pd_database\n",
    "\n",
    "def process_skeleton(data_folder,save_path):\n",
    "    pd_database = process_folders(data_folder)\n",
    "    pd_database = pd.concat(pd_database)\n",
    "    pd_database.to_hdf(save_path, key='df', mode='w')\n",
    "\n",
    "process_skeleton('/work/21010294/HandGesture/Dataset/','skeleton.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load skeleton.h5 and display\n",
    "df = pd.read_hdf('skeleton.h5')\n",
    "#print(df['landmarks'][df['fid'] == '0013'].to_numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training skeleton model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train set size: 71431\n",
      "Validation set size: 21980\n",
      "Test set size: 31949\n",
      "Epoch 1/20, Validation Loss: 0.0398, Validation Accuracy: 63.22%\n",
      "Epoch 2/20, Validation Loss: 0.0382, Validation Accuracy: 68.26%\n",
      "Epoch 3/20, Validation Loss: 0.0374, Validation Accuracy: 70.84%\n",
      "Epoch 4/20, Validation Loss: 0.0377, Validation Accuracy: 69.78%\n",
      "Epoch 5/20, Validation Loss: 0.0376, Validation Accuracy: 70.13%\n",
      "Epoch 6/20, Validation Loss: 0.0372, Validation Accuracy: 71.23%\n",
      "Epoch 7/20, Validation Loss: 0.0373, Validation Accuracy: 70.88%\n",
      "Epoch 8/20, Validation Loss: 0.0372, Validation Accuracy: 71.06%\n",
      "Epoch 9/20, Validation Loss: 0.0372, Validation Accuracy: 71.13%\n",
      "Epoch 10/20, Validation Loss: 0.0372, Validation Accuracy: 71.13%\n",
      "Epoch 11/20, Validation Loss: 0.0372, Validation Accuracy: 71.11%\n",
      "Epoch 12/20, Validation Loss: 0.0372, Validation Accuracy: 71.13%\n",
      "Epoch 13/20, Validation Loss: 0.0372, Validation Accuracy: 71.13%\n",
      "Epoch 14/20, Validation Loss: 0.0372, Validation Accuracy: 71.13%\n",
      "Epoch 15/20, Validation Loss: 0.0372, Validation Accuracy: 71.13%\n",
      "Epoch 16/20, Validation Loss: 0.0372, Validation Accuracy: 71.13%\n",
      "Epoch 17/20, Validation Loss: 0.0372, Validation Accuracy: 71.13%\n",
      "Epoch 18/20, Validation Loss: 0.0372, Validation Accuracy: 71.13%\n",
      "Epoch 19/20, Validation Loss: 0.0372, Validation Accuracy: 71.13%\n",
      "Epoch 20/20, Validation Loss: 0.0372, Validation Accuracy: 71.13%\n",
      "Test Loss: 0.0343, Test Accuracy: 80.54%\n",
      "Precision: 0.81, Recall: 0.81, F1 Score: 0.80\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      6173\n",
      "           1       0.89      0.97      0.93      6117\n",
      "           2       0.96      0.87      0.91      6081\n",
      "           3       0.65      0.64      0.64      7835\n",
      "           4       0.60      0.60      0.60      5743\n",
      "\n",
      "    accuracy                           0.81     31949\n",
      "   macro avg       0.81      0.81      0.81     31949\n",
      "weighted avg       0.81      0.81      0.80     31949\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_hdf('skeleton.h5')\n",
    "# Custom Dataset class\n",
    "class HandPostureDataset(Dataset):\n",
    "    def __init__(self, dataframe,flip_hand=True,angle_hand=False):\n",
    "        self.data = dataframe['landmarks'].values\n",
    "        self.handness = dataframe['handness'].values\n",
    "        self.labels = dataframe['labels'].values  \n",
    "        self.flip_hand = flip_hand\n",
    "        self.angle_hand = angle_hand\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        sample = self.feature_extraction(sample,self.handness[idx])\n",
    "        #normalize to 0-1\n",
    "        sample = (sample - sample.min()) / (sample.max() - sample.min())\n",
    "        #print(sample.shape)\n",
    "        sample = torch.tensor(sample, dtype=torch.float)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return sample, label\n",
    "    def feature_extraction(self,landmarks,handness):\n",
    "        PARSE_LANDMARKS_JOINTS = [    \n",
    "        [0, 1], [1, 2], [2, 3], [3, 4], # thumb\n",
    "        [0, 5],[5, 6], [6, 7], [7, 8], # index finger\n",
    "        [5, 9],[9,10],[10, 11], [11, 12],# middle finger\n",
    "        [9, 13],[13, 14],[14, 15],[15, 16], # ring finger\n",
    "        [13, 17],  [17, 18], [18, 19], [19,20]   # little finger\n",
    "        ]   \n",
    "        def calculate_angle(landmark1, landmark2):\n",
    "            return np.math.atan2(np.linalg.det([landmark1, landmark2]), np.dot(landmark1, landmark2))\n",
    "        if self.flip_hand and handness == 1:\n",
    "            landmarks = landmarks * np.array([-1,1])\n",
    "        if self.angle_hand:\n",
    "            angles = []\n",
    "            for joint in PARSE_LANDMARKS_JOINTS:\n",
    "                angle = calculate_angle(landmarks[joint[0]],landmarks[joint[1]])\n",
    "                angles.append(angle)\n",
    "            return np.array([angles,angles]).T\n",
    "        return landmarks\n",
    "            \n",
    "\n",
    "\n",
    "# Model definition\n",
    "class HandPostureModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandPostureModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=100, batch_first=True)\n",
    "        self.fc1 = nn.Linear(100, 50)\n",
    "        self.fc2 = nn.Linear(50, len(Actions))  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  \n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :] \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_hdf('skeleton.h5')\n",
    "\n",
    "\n",
    "# Manually set test and validation IDs\n",
    "test_ids = ['1584', '0246', '0110', '0013']\n",
    "val_ids = ['2089', '1596']\n",
    "\n",
    "# Get unique IDs\n",
    "unique_fids = df['fid'].unique()\n",
    "\n",
    "# Determine training IDs by excluding test and validation IDs\n",
    "train_ids = [fid for fid in unique_fids if fid not in test_ids + val_ids]\n",
    "\n",
    "# Create dataframes for each set\n",
    "train_df = df[df['fid'].isin(train_ids)]\n",
    "val_df = df[df['fid'].isin(val_ids)]\n",
    "test_df = df[df['fid'].isin(test_ids)]\n",
    "\n",
    "# Verify the splits\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "\n",
    "train_dataset = HandPostureDataset(train_df)\n",
    "val_dataset = HandPostureDataset(val_df)\n",
    "test_dataset = HandPostureDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = HandPostureModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = 100. * correct / len(val_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Test the model and calculate metrics\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs, labels).item()\n",
    "        pred = outputs.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "print(f'Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}')\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Export results to a CSV file\n",
    "results = {\n",
    "    'Test Loss': [test_loss],\n",
    "    'Test Accuracy': [test_accuracy],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1 Score': [f1]\n",
    "}\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('test_results.csv', index=False)\n",
    "#save model\n",
    "torch.save(model.state_dict(), 'hand_posture_model.pth')\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and dataset creation (images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 63595.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/21010294/HandGesture/Dataset/21011611_VuDuongKhang\n",
      "/work/21010294/HandGesture/Dataset/VepleyAI_dataset_23010111_tongnguyenbaolong\n",
      "/work/21010294/HandGesture/Dataset/VepleyAI_dataset_22010117_PhungHoangAnh\n",
      "/work/21010294/HandGesture/Dataset/21013192_NongNgocHuan\n",
      "/work/21010294/HandGesture/Dataset/22010952_Nguyenvanbang\n",
      "/work/21010294/HandGesture/Dataset/23010570_TranBuiNguyenDuong\n",
      "/work/21010294/HandGesture/Dataset/22010057_VuQuangDuong\n",
      "/work/21010294/HandGesture/Dataset/VepleyAI_dataset_21012089_NguyenAnhQuan\n",
      "/work/21010294/HandGesture/Dataset/22010402_NGUYENHUYHOANG\n",
      "/work/21010294/HandGesture/Dataset/VepleyAI_dataset_21010610_Phan_Huy_Duong\n",
      "/work/21010294/HandGesture/Dataset/VepleyAI_dataset_22010127_NguyenHuuTan\n",
      "/work/21010294/HandGesture/Dataset/21012089_NguyenAnhQuan\n",
      "/work/21010294/HandGesture/Dataset/VepleyAI_dataset_21011596_PhanMinhDuc\n",
      "/work/21010294/HandGesture/Dataset/22010246_NguyenTheDuy\n",
      "/work/21010294/HandGesture/Dataset/VepleyAI_dataset_21011584_TranManhCuong\n",
      "/work/21010294/HandGesture/Dataset/22010013_PhamThanhBinh\n",
      "/work/21010294/HandGesture/Dataset/VepleyAI_dataset_22010110_DoTuanDat\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_folder(folder,save_path,type='train'):\n",
    "    print(folder)\n",
    "    folder = str(folder)\n",
    "    if not len(os.listdir(folder)):\n",
    "        return None\n",
    "    elif len(os.listdir(folder)) == 1:\n",
    "        folder = os.path.join(folder, str(os.listdir(folder)[0]))\n",
    "    #print(f'Processing folder {folder}')\n",
    "    base_name = os.path.basename(folder)\n",
    "    #process json\n",
    "    fid = str(extract_numbers(base_name)[0])[-4:]\n",
    "    #print(os.listdir(folder))\n",
    "    \n",
    "    for c in os.listdir(folder):\n",
    "        #if f is folder\n",
    "        if os.path.isdir(os.path.join(folder, c)):\n",
    "            os.makedirs(os.path.join(save_path,type,c),exist_ok=True)\n",
    "            for file in os.listdir(os.path.join(folder, c)):\n",
    "                if file.endswith('.jpg') and '_drawed.jpg' not in file:\n",
    "                    shutil.copy(os.path.join(folder, c, file),os.path.join(save_path,type,c,f\"{fid}_{file}\"))\n",
    "            \n",
    "\n",
    "\n",
    "def process_imgs(data_folder,save_path):\n",
    "    folders = []\n",
    "    #fid = str(extract_numbers(base_name)[0])[-4:]\n",
    "    for folder in tqdm(os.listdir(data_folder)):\n",
    "        #if folder is a folder\n",
    "        if os.path.isdir(os.path.join(data_folder, folder)) and not '[discarded]' in folder:\n",
    "            folders.append(os.path.join(data_folder, folder))\n",
    "    #.7 folder train, .1 folder val, .2 folder test\n",
    "    np.random.seed(12)\n",
    "    #np.random.shuffle(folders)\n",
    "    train_folders, val_folders, test_folders = np.split(folders, [int(.7*len(folders)), int(.8*len(folders))])\n",
    "    for folder in list(train_folders):\n",
    "        process_folder(folder,save_path,type='train')\n",
    "    for folder in list(val_folders):\n",
    "        process_folder(folder,save_path,type='val')\n",
    "    for folder in list(test_folders):\n",
    "        process_folder(folder,save_path,type='test')\n",
    "    print('Done')\n",
    "    \n",
    "\n",
    "\n",
    "process_imgs('/work/21010294/HandGesture/Dataset/','imgs_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs_data 0\n",
      "imgs_data/test 0\n",
      "imgs_data/test/Pickup_item 3099\n",
      "imgs_data/test/Use_item 3100\n",
      "imgs_data/test/Aim 4100\n",
      "imgs_data/test/Shoot 3099\n",
      "imgs_data/test/Idle 3100\n",
      "imgs_data/train 0\n",
      "imgs_data/train/Pickup_item 9096\n",
      "imgs_data/train/Use_item 7097\n",
      "imgs_data/train/Aim 7095\n",
      "imgs_data/train/Shoot 7198\n",
      "imgs_data/train/Idle 8098\n",
      "imgs_data/val 0\n",
      "imgs_data/val/Pickup_item 1999\n",
      "imgs_data/val/Use_item 1999\n",
      "imgs_data/val/Aim 1999\n",
      "imgs_data/val/Shoot 3999\n",
      "imgs_data/val/Idle 1998\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk('imgs_data'):\n",
    "    print(root, len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training images model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/21010294/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/21010294/VSR/VSREnv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/21010294/VSR/VSREnv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: L=0.0000: 100%|██████████| 1206/1206 [05:28<00:00,  3.67it/s]\n",
      "Epoch:   5%|▌         | 1/20 [06:48<2:09:14, 408.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Validation Loss: 0.0840, Validation Accuracy: 42.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: L=0.0000:  63%|██████▎   | 761/1206 [03:10<01:51,  4.00it/s]\n",
      "Epoch:   5%|▌         | 1/20 [09:58<3:09:31, 598.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     46\u001b[0m disp_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader,desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: L=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisp_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     48\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     49\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/VSR/VSREnv/lib/python3.9/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/VSR/VSREnv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/VSR/VSREnv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/VSR/VSREnv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/VSR/VSREnv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/VSR/VSREnv/lib/python3.9/site-packages/torchvision/datasets/folder.py:232\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    230\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/VSR/VSREnv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 94\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/VSR/VSREnv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VSR/VSREnv/lib/python3.9/site-packages/torchvision/transforms/functional.py:170\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    168\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(pic\u001b[38;5;241m.\u001b[39mgetbands()))\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "base_model = 'resnet50'\n",
    "\n",
    "if base_model == 'resnet50':\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "    #transfer learning to 5 classes\n",
    "    model.fc = nn.Linear(model.fc.in_features, len(Actions))\n",
    "elif base_model == 'mobilenet_v2':\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, len(Actions))\n",
    "elif base_model == 'vgg16':\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\n",
    "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, len(Actions))\n",
    "elif base_model == 'densenet121':\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n",
    "    model.classifier = nn.Linear(model.classifier.in_features, len(Actions))\n",
    "else:\n",
    "    print('Invalid model specified')\n",
    "    exit()\n",
    "\n",
    "#read dataset\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_dataset = ImageFolder('imgs_data/train', transform=data_transform)\n",
    "val_dataset = ImageFolder('imgs_data/val', transform=data_transform)\n",
    "test_dataset = ImageFolder('imgs_data/test', transform=data_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in tqdm(range(num_epochs),desc='Epoch'):\n",
    "    model.train()\n",
    "    disp_loss = 0\n",
    "    for inputs, labels in tqdm(train_loader,desc=f\"Epoch {epoch+1}/{num_epochs}: L={disp_loss:.4f}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        disp_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = 100. * correct / len(val_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Test the model and calculate metrics\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs, labels).item()\n",
    "        pred = outputs.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "print(f'Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}')\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and dataset creation (hand images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_folder(folder,save_path,type='train'):\n",
    "    if not len(os.listdir(folder)):\n",
    "        return None\n",
    "    elif len(os.listdir(folder)) == 1:\n",
    "        folder = os.path.join(folder, os.listdir(folder)[0])\n",
    "    #print(f'Processing folder {folder}')\n",
    "    base_name = os.path.basename(folder)\n",
    "    #process json\n",
    "    fid = str(extract_numbers(base_name)[0])[-4:]\n",
    "    #print(os.listdir(folder))\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith('.json') and 'detail' not in file:\n",
    "            landmarks, handness, labels = get_skeleton_json(os.path.join(folder, file))\n",
    "            if landmarks is None:\n",
    "                continue\n",
    "            #append to pd_database with columns: landmarks, handness, labels, fid\n",
    "            pd_database.append(pd.DataFrame({'landmarks':landmarks,\n",
    "                                             'handness':handness,\n",
    "                                             'labels':labels,\n",
    "                                             'fid':fid}))\n",
    "            #print(f'Processing {file}')\n",
    "    return pd_database\n",
    "def process_folder(folder,save_path,type='train'):\n",
    "    print(folder)\n",
    "    folder = str(folder)\n",
    "    if not len(os.listdir(folder)):\n",
    "        return None\n",
    "    elif len(os.listdir(folder)) == 1:\n",
    "        folder = os.path.join(folder, str(os.listdir(folder)[0]))\n",
    "    #print(f'Processing folder {folder}')\n",
    "    base_name = os.path.basename(folder)\n",
    "    #process json\n",
    "    fid = str(extract_numbers(base_name)[0])[-4:]\n",
    "    #print(os.listdir(folder))\n",
    "    \n",
    "    for c in os.listdir(folder):\n",
    "        #if f is folder\n",
    "        if os.path.isdir(os.path.join(folder, c)):\n",
    "            if not os.path.exists(os.path.join(folder, f\"{c}.json\")):\n",
    "                continue\n",
    "            bounding_box_file = os.path.join(folder, f\"{c}.json\")\n",
    "            os.makedirs(os.path.join(save_path,type,c),exist_ok=True)\n",
    "            for file in os.listdir(os.path.join(folder, c)):\n",
    "                if file.endswith('.jpg') and '_drawed.jpg' not in file:\n",
    "                    shutil.copy(os.path.join(folder, c, file),os.path.join(save_path,type,c,f\"{fid}_{file}\"))\n",
    "\n",
    "\n",
    "def process_imgs(data_folder,save_path):\n",
    "    folders = []\n",
    "    #fid = str(extract_numbers(base_name)[0])[-4:]\n",
    "    for folder in tqdm(os.listdir(data_folder)):\n",
    "        #if folder is a folder\n",
    "        if os.path.isdir(os.path.join(data_folder, folder)) and not '[discarded]' in folder:\n",
    "            folders.append(os.path.join(data_folder, folder))\n",
    "    #.7 folder train, .1 folder val, .2 folder test\n",
    "    np.random.seed(12)\n",
    "    #np.random.shuffle(folders)\n",
    "    train_folders, val_folders, test_folders = np.split(folders, [int(.7*len(folders)), int(.8*len(folders))])\n",
    "    for folder in list(train_folders):\n",
    "        process_folder(folder,save_path,type='train')\n",
    "    for folder in list(val_folders):\n",
    "        process_folder(folder,save_path,type='val')\n",
    "    for folder in list(test_folders):\n",
    "        process_folder(folder,save_path,type='test')\n",
    "    print('Done')\n",
    "    \n",
    "\n",
    "\n",
    "process_imgs('/work/21010294/HandGesture/Dataset/','imgs_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train late fusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from PIL import Image\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "#load hand_posture_model.pth\n",
    "class HandPostureDataset(Dataset):\n",
    "    def __init__(self, dataframe,imgs_path,flip_hand=True,angle_hand=False):\n",
    "        self.data = dataframe['landmarks'].values\n",
    "        self.handness = dataframe['handness'].values\n",
    "        self.labels = dataframe['labels'].values  \n",
    "        self.frame_ids = dataframe['frame_ids'].values\n",
    "        self.fid = dataframe['fid'].values\n",
    "        self.flip_hand = flip_hand\n",
    "        self.angle_hand = angle_hand\n",
    "        self.imgs_path = imgs_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        sample = self.feature_extraction(sample,self.handness[idx])\n",
    "        #normalize to 0-1\n",
    "        sample = (sample - sample.min()) / (sample.max() - sample.min())\n",
    "        #print(sample.shape)\n",
    "        sample = torch.tensor(sample, dtype=torch.float)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        sample_frame_name = os.path.join(self.imgs_path,Actions[self.labels[idx]],f\"{self.fid[idx]}_{self.frame_ids[idx]}.jpg\")\n",
    "        \n",
    "        img = Image.open(sample_frame_name)\n",
    "        img = data_transform(img)\n",
    "        return sample, img, label\n",
    "        \n",
    "\n",
    "    def feature_extraction(self,landmarks,handness):\n",
    "        PARSE_LANDMARKS_JOINTS = [    \n",
    "        [0, 1], [1, 2], [2, 3], [3, 4], # thumb\n",
    "        [0, 5],[5, 6], [6, 7], [7, 8], # index finger\n",
    "        [5, 9],[9,10],[10, 11], [11, 12],# middle finger\n",
    "        [9, 13],[13, 14],[14, 15],[15, 16], # ring finger\n",
    "        [13, 17],  [17, 18], [18, 19], [19,20]   # little finger\n",
    "        ]   \n",
    "        def calculate_angle(landmark1, landmark2):\n",
    "            return np.math.atan2(np.linalg.det([landmark1, landmark2]), np.dot(landmark1, landmark2))\n",
    "        if self.flip_hand and handness == 1:\n",
    "            landmarks = landmarks * np.array([-1,1])\n",
    "        if self.angle_hand:\n",
    "            angles = []\n",
    "            for joint in PARSE_LANDMARKS_JOINTS:\n",
    "                angle = calculate_angle(landmarks[joint[0]],landmarks[joint[1]])\n",
    "                angles.append(angle)\n",
    "            return np.array([angles,angles]).T\n",
    "        return landmarks\n",
    "            \n",
    "\n",
    "\n",
    "# Model definition\n",
    "class HandPostureModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HandPostureModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=100, batch_first=True)\n",
    "        self.fc1 = nn.Linear(100, 50)\n",
    "        self.fc2 = nn.Linear(50, len(Actions))  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  \n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :] \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "    \n",
    "model_hand = HandPostureModel()\n",
    "model_hand.load_state_dict(torch.load('hand_posture_model.pth'))\n",
    "\n",
    "#to fit with late fusion withh image, change fc2  from 5 to 1000\n",
    "model_hand.fc2 = nn.Linear(50, 1000)\n",
    "base_model = 'resnet50'\n",
    "\n",
    "if base_model == 'resnet50':\n",
    "    model_image = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "    #transfer learning to 5 classes\n",
    "    model_image.fc = nn.Linear(model_image.fc.in_features, len(Actions))\n",
    "    #load 'resnet50.pth'\n",
    "    model_image.load_state_dict(torch.load('resnet50.pth'))\n",
    "\n",
    "    model_image.fc = nn.Linear(model_image.fc.in_features, 1000)\n",
    "elif base_model == 'mobilenet_v2':\n",
    "    model_image = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "    model_image.classifier[1] = nn.Linear(model_image.classifier[1].in_features, len(Actions))\n",
    "    #load 'mobilenet_v2.pth'\n",
    "    model_image.load_state_dict(torch.load('mobilenet_v2.pth'))\n",
    "    model_image.classifier[1] = nn.Linear(model_image.classifier[1].in_features, 1000)\n",
    "elif base_model == 'vgg16':\n",
    "    model_image = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\n",
    "    model_image.classifier[6] = nn.Linear(model_image.classifier[6].in_features, len(Actions))\n",
    "    #load 'vgg16.pth'\n",
    "    model_image.load_state_dict(torch.load('vgg16.pth'))\n",
    "    model_image.classifier[6] = nn.Linear(model_image.classifier[6].in_features, 1000)\n",
    "elif base_model == 'densenet121':\n",
    "    model_image = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n",
    "    model_image.classifier = nn.Linear(model_image.classifier.in_features, len(Actions))\n",
    "    #load 'densenet121.pth'\n",
    "    model_image.load_state_dict(torch.load('densenet121.pth'))\n",
    "    model_image.classifier = nn.Linear(model_image.classifier.in_features, 1000)\n",
    "else:\n",
    "    print('Invalid model specified')\n",
    "    exit()\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_hdf('skeleton.h5')\n",
    "\n",
    "\n",
    "# Manually set test and validation IDs\n",
    "test_ids = ['1584', '0246', '0110', '0013']\n",
    "val_ids = ['2089', '1596']\n",
    "\n",
    "# Get unique IDs\n",
    "unique_fids = df['fid'].unique()\n",
    "\n",
    "# Determine training IDs by excluding test and validation IDs\n",
    "train_ids = [fid for fid in unique_fids if fid not in test_ids + val_ids]\n",
    "\n",
    "# Create dataframes for each set\n",
    "train_df = df[df['fid'].isin(train_ids)]\n",
    "val_df = df[df['fid'].isin(val_ids)]\n",
    "test_df = df[df['fid'].isin(test_ids)]\n",
    "\n",
    "# Verify the splits\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "\n",
    "train_dataset = HandPostureDataset(train_df,'imgs_data/train/')\n",
    "val_dataset = HandPostureDataset(val_df,'imgs_data/val/')\n",
    "test_dataset = HandPostureDataset(test_df,'imgs_data/test/')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "#init a model with late fusion\n",
    "class LateFusionModel(nn.Module):\n",
    "    def __init__(self, model_hand, model_image):\n",
    "        super(LateFusionModel, self).__init__()\n",
    "        self.model_hand = model_hand\n",
    "        self.model_image = model_image\n",
    "        self.fc = nn.Linear(2000, len(Actions))\n",
    "\n",
    "    def forward(self, x_hand, x_image):\n",
    "        x_hand = self.model_hand(x_hand)\n",
    "        x_image = self.model_image(x_image)\n",
    "        x = torch.cat((x_hand, x_image), dim=1)\n",
    "        x = self.fc(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "    \n",
    "model = LateFusionModel(model_hand, model_image).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs_hand, inputs_image, labels in train_loader:\n",
    "        inputs_hand, inputs_image, labels = inputs_hand.to(device), inputs_image.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs_hand, inputs_image)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs_hand, inputs_image, labels in val_loader:\n",
    "            inputs_hand, inputs_image, labels = inputs_hand.to(device), inputs_image.to(device), labels.to(device)\n",
    "            outputs = model(inputs_hand, inputs_image)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = 100. * correct / len(val_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Test the model and calculate metrics\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for inputs_hand, inputs_image, labels in test_loader:\n",
    "        inputs_hand, inputs_image, labels = inputs_hand.to(device), inputs_image.to(device), labels.to(device)\n",
    "        outputs = model(inputs_hand, inputs_image)\n",
    "        test_loss += criterion(outputs, labels).item()\n",
    "        pred = outputs.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "print(f'Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}')\n",
    "print(classification_report(all_labels, all_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
